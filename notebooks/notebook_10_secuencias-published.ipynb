{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f023bae",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_10_secuencias-published.ipynb)\n",
    "\n",
    "# Prediciendo secuencias con redes neuronales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9649739",
   "metadata": {},
   "source": [
    "En este notebook vamos a abordar el problema de traducci√≥n autom√°tica de secuencias, espec√≠ficamente la traducci√≥n de oraciones del ingl√©s al espa√±ol.\n",
    "\n",
    "El primer paso fundamental para resolver este problema es contar con un dataset adecuado. Para ello, podemos explorar la plataforma ü§ó Hugging Face Datasets, que ofrece una amplia colecci√≥n de conjuntos de datos etiquetados y listos para usar. En particular, se puede filtrar por la tarea de Translation para encontrar datasets que contengan pares de oraciones en distintos idiomas, incluyendo espa√±ol e ingl√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f1fdaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuraci√≥n del dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "torch.cuda.set_per_process_memory_fraction(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f0b578",
   "metadata": {},
   "source": [
    "### 1. Preparaci√≥n de Datos\n",
    "\n",
    "Vamos a utilizar el dataset [`google/wmt24pp`](https://huggingface.co/datasets/google/wmt24pp). Para este ejercicio, trabajaremos con el par `en-es_MX` (ingl√©s a espa√±ol de M√©xico).\n",
    "\n",
    "Tienen que armar una funci√≥n que devuelva:\n",
    "- Una lista con las oraciones en ingl√©s (`source`)\n",
    "- Una lista con sus correspondientes traducciones en espa√±ol (`target`)\n",
    "\n",
    "Solo deben incluirse ejemplos que **no est√©n marcados como de baja calidad** (`is_bad_source == false`).\n",
    "\n",
    "> üí° Leer el [dataset card](https://huggingface.co/datasets/google/wmt24pp) para entender el formato de los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bdfa84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Cargamos el dataset para el par en-es_MX\n",
    "dataset = load_dataset(\"google/wmt24pp\", \"en-es_MX\", split=\"train\")\n",
    "\n",
    "def obtener_listas_oraciones(dataset):\n",
    "    oraciones_en, oraciones_es = [], []\n",
    "    for data in dataset:\n",
    "        if not data['is_bad_source']:\n",
    "            oraciones_en.append(data['source'])\n",
    "            oraciones_es.append(data['target'])\n",
    "            \n",
    "    return oraciones_en, oraciones_es\n",
    "oraciones_en, oraciones_es = obtener_listas_oraciones(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60b45e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Siso's depictions of land, water center new gallery exhibition\",\n",
       " '\"People Swimming in the Swimming Pool\" from 2022 is one Vicente Siso artwork that will display at Tierra del Sol Gallery beginning Jan. 13. (photo courtesy of Vicente Siso)',\n",
       " 'Tierra del Sol is pleased to present \"Vicente Siso: Memories of the Land and Water\" at the new gallery location in West Hollywood. Siso has been an artist in the Studio Arts Program since 2012, this marks his debut solo exhibition. Siso was born 1962 in Madrid and raised between Venezuela, Trinidad and Miami; he moved with his family to Southern California in his early 20s.',\n",
       " 'Masterfully working across subject matter, Siso has generated a prolific series of landscapes, portraits, and still-life works rendered in either acrylic, pastel, pencil or watercolor. Drawing from family portraits, his own reference photographs, and recollection, his colorful compositions demonstrate his range of interests and skill across media. Siso\\'s tropical landscapes and seascapes reflect the geographies of his past, employing rich patterns and incorporating people to make meaningful connections between culture, memory and the environment. Siso titles his artworks in a mix of Spanish and English, signifying the celebrated and integral complexities of his life in Los Angeles County. \"Vicente Siso: Memories of the Land and Water\" opens on Saturday, Jan. 13, with a reception from 6-8 p.m. The exhibition is on view through Sunday, March 3.',\n",
       " 'The Tierra del Sol Gallery is located at 7414 Santa Monica Blvd. For information, visit tierradelsolgallery.org.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d1674a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Representaciones de la tierra y el agua de Siso son el centro de una nueva exposici√≥n',\n",
       " '‚ÄúGente nadando en la alberca‚Äù (2022) es una de las obras de Vicente Siso que se expondr√°n en la galer√≠a Tierra del Sol a partir del 13 de enero (foto cortes√≠a de Vicente Siso)',\n",
       " 'Tierra del Sol se complace en presentar ‚ÄúVicente Siso: Recuerdos de la tierra y el agua‚Äù en la nueva ubicaci√≥n de la galer√≠a, en West Hollywood. Siso es un artista del Studio Arts Program desde 2012 y esta exposici√≥n marca su debut como solista. Nacido en Madrid en 1962, creci√≥ alternando entre Venezuela, Trinidad y Miami. Con unos 20 a√±os, se mud√≥ con su familia al sur de California.',\n",
       " 'Trabajando con maestr√≠a diversas tem√°ticas, Siso ha creado una prol√≠fica serie de paisajes, retratos y naturalezas muertas en acr√≠lico, pastel, l√°piz y acuarela. A partir de retratos familiares, de sus propias fotograf√≠as referenciales y de sus recuerdos, sus coloridas composiciones demuestran una gran amplitud de intereses y habilidad con los distintos medios. Los paisajes tropicales y marinos de Siso reflejan las geograf√≠as de su pasado, empleando ricos patrones e incorporando personas para crear conexiones significativas entre la cultura, la memoria y el medio ambiente. Siso da t√≠tulo a sus obras en una mezcla de espa√±ol e ingl√©s, evidenciando la notoria complejidad integral de su vida en el condado de Los √Ångeles. ‚ÄúVicente Siso: Recuerdos de la tierra y el agua‚Äù abre al p√∫blico el s√°bado 13 de enero con una recepci√≥n de 6 a 8 p.m. La exposici√≥n permanecer√° abierta hasta el domingo 3 de marzo, inclusive.',\n",
       " 'La galer√≠a Tierra del Sol se encuentra en el 7414 de Santa M√≥nica Blvd. Para m√°s informaci√≥n, visita tierradelsolgallery.org.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_es[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb289953",
   "metadata": {},
   "source": [
    "Adem√°s, **vamos a preprocesar los datos** para aplicar una limpieza b√°sica a los textos. Para eso, aplicaremos una funci√≥n `preprocess_text` que:\n",
    "- Pase el texto a min√∫sculas\n",
    "- Elimine puntuaci√≥n innecesaria, pero conserve `.,!?¬ø¬°`\n",
    "- Elimine espacios redundantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b31de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesa el texto para limpieza b√°sica\"\"\"\n",
    "    # Limpiar pero mantener puntuaci√≥n b√°sica\n",
    "    text = text.strip()\n",
    "    # Convertir a min√∫sculas\n",
    "    text = text.lower()\n",
    "    # Remover puntuaci√≥n excesiva pero mantener puntos y comas\n",
    "    text = re.sub(r'[^\\w\\s.,!?¬ø¬°]', '', text)\n",
    "    # Remover espacios extra\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a754574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de pares: 960, 960\n",
      "\n",
      "Ejemplos:\n",
      "  EN: sisos depictions of land, water center new gallery exhibition\n",
      "  ES: representaciones de la tierra y el agua de siso son el centro de una nueva exposici√≥n\n",
      "\n",
      "  EN: people swimming in the swimming pool from 2022 is one vicente siso artwork that will display at tierra del sol gallery beginning jan. 13. photo courtesy of vicente siso\n",
      "  ES: gente nadando en la alberca 2022 es una de las obras de vicente siso que se expondr√°n en la galer√≠a tierra del sol a partir del 13 de enero foto cortes√≠a de vicente siso\n",
      "\n",
      "  EN: tierra del sol is pleased to present vicente siso memories of the land and water at the new gallery location in west hollywood. siso has been an artist in the studio arts program since 2012, this marks his debut solo exhibition. siso was born 1962 in madrid and raised between venezuela, trinidad and miami he moved with his family to southern california in his early 20s.\n",
      "  ES: tierra del sol se complace en presentar vicente siso recuerdos de la tierra y el agua en la nueva ubicaci√≥n de la galer√≠a, en west hollywood. siso es un artista del studio arts program desde 2012 y esta exposici√≥n marca su debut como solista. nacido en madrid en 1962, creci√≥ alternando entre venezuela, trinidad y miami. con unos 20 a√±os, se mud√≥ con su familia al sur de california.\n",
      "\n",
      "  EN: masterfully working across subject matter, siso has generated a prolific series of landscapes, portraits, and stilllife works rendered in either acrylic, pastel, pencil or watercolor. drawing from family portraits, his own reference photographs, and recollection, his colorful compositions demonstrate his range of interests and skill across media. sisos tropical landscapes and seascapes reflect the geographies of his past, employing rich patterns and incorporating people to make meaningful connections between culture, memory and the environment. siso titles his artworks in a mix of spanish and english, signifying the celebrated and integral complexities of his life in los angeles county. vicente siso memories of the land and water opens on saturday, jan. 13, with a reception from 68 p.m. the exhibition is on view through sunday, march 3.\n",
      "  ES: trabajando con maestr√≠a diversas tem√°ticas, siso ha creado una prol√≠fica serie de paisajes, retratos y naturalezas muertas en acr√≠lico, pastel, l√°piz y acuarela. a partir de retratos familiares, de sus propias fotograf√≠as referenciales y de sus recuerdos, sus coloridas composiciones demuestran una gran amplitud de intereses y habilidad con los distintos medios. los paisajes tropicales y marinos de siso reflejan las geograf√≠as de su pasado, empleando ricos patrones e incorporando personas para crear conexiones significativas entre la cultura, la memoria y el medio ambiente. siso da t√≠tulo a sus obras en una mezcla de espa√±ol e ingl√©s, evidenciando la notoria complejidad integral de su vida en el condado de los √°ngeles. vicente siso recuerdos de la tierra y el agua abre al p√∫blico el s√°bado 13 de enero con una recepci√≥n de 6 a 8 p.m. la exposici√≥n permanecer√° abierta hasta el domingo 3 de marzo, inclusive.\n",
      "\n",
      "  EN: the tierra del sol gallery is located at 7414 santa monica blvd. for information, visit tierradelsolgallery.org.\n",
      "  ES: la galer√≠a tierra del sol se encuentra en el 7414 de santa m√≥nica blvd. para m√°s informaci√≥n, visita tierradelsolgallery.org.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocesar datos\n",
    "oraciones_en = [preprocess_text(sent) for sent in oraciones_en if sent.strip()]\n",
    "oraciones_es = [preprocess_text(sent) for sent in oraciones_es if sent.strip()]\n",
    "\n",
    "print(f\"Total de pares: {len(oraciones_en)}, {len(oraciones_es)}\")\n",
    "print(\"\\nEjemplos:\")\n",
    "for i in range(min(5, len(oraciones_en))):\n",
    "    print(f\"  EN: {oraciones_en[i]}\")\n",
    "    print(f\"  ES: {oraciones_es[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad6f1a",
   "metadata": {},
   "source": [
    "Para terminar, **qued√°ndonos con los conjuntos de entrenamiento y validaci√≥n**, vamos a dividir los pares de oraciones utilizando `train_test_split` de `sklearn`.\n",
    "\n",
    "üìå Separamos un 80‚ÄØ% para entrenamiento y un 20‚ÄØ% para validaci√≥n. Usamos `random_state=42` para asegurar reproducibilidad.\n",
    "\n",
    "üí° Si lo desean, pueden experimentar con otras formas de dividir al conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "242955b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 768 pares\n",
      "Validaci√≥n: 192 pares\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_eng, val_eng, train_esp, val_esp = train_test_split(\n",
    "    oraciones_en, oraciones_es, test_size=0.2, random_state=28\n",
    ")\n",
    "\n",
    "print(f\"Entrenamiento: {len(train_eng)} pares\")\n",
    "print(f\"Validaci√≥n: {len(val_eng)} pares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a3b7c4",
   "metadata": {},
   "source": [
    "### 2. Construcci√≥n de Vocabularios\n",
    "\n",
    "Antes de entrenar nuestro modelo, necesitamos convertir las oraciones en secuencias de n√∫meros. Para eso, vamos a construir un **vocabulario para cada idioma** (ingl√©s y espa√±ol), que asigne un √≠ndice √∫nico a cada palabra.\n",
    "\n",
    "üí° Tambi√©n incluimos **tokens especiales**:\n",
    "- `SOS`: indica el inicio de una oraci√≥n,\n",
    "- `EOS`: indica el final,\n",
    "- `PAD`: se usar√° para rellenar oraciones cortas hasta una longitud uniforme.\n",
    "\n",
    "Cada oraci√≥n del conjunto de entrenamiento se usa para agregar palabras al vocabulario correspondiente. De esta forma, el modelo solo trabajar√° con palabras que haya visto durante el entrenamiento.\n",
    "\n",
    "Implementar una clase `Vocabulario` que:\n",
    "- Guarde los mapeos palabra‚Äì√≠ndice y viceversa,\n",
    "- Mantenga un contador de palabras,\n",
    "- Tenga m√©todos para agregar palabras y oraciones.\n",
    "\n",
    "Una vez implementada, usala para construir los vocabularios con los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "767a2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulario:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.palab_idx = {'SOS': 0, 'EOS': 1, 'PAD': 2, '.':3, ',':4, '!':5, '?':6, '¬ø':7,'¬°':8, '<UNK>': 9}\n",
    "        self.ind_palab = ['SOS', 'EOS', 'PAD', '.', ',', '!', '?', '¬ø','¬°', '<UNK>']\n",
    "        self.n = len(self.ind_palab)\n",
    "\n",
    "    def add_palabra(self, palabra):\n",
    "        self.palab_idx[palabra] = self.n\n",
    "        self.ind_palab.append(palabra)\n",
    "        self.n += 1\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        for palabra in sentence.split():\n",
    "            if not palabra in self.ind_palab:\n",
    "                self.add_palabra(palabra)\n",
    "                \n",
    "    def n_words(self):\n",
    "        return self.n\n",
    "    \n",
    "    def get_palabra(self, idx):\n",
    "        return self.ind_palab[idx]\n",
    "    \n",
    "    def get_index(self, palabra):\n",
    "        return self.palab_idx[palabra]\n",
    "    \n",
    "    def is_in_palabra(self, palabra):\n",
    "        return palabra in self.palab_idx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce387ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario ingl√©s: <bound method Vocabulario.n_words of <__main__.Vocabulario object at 0x0000025156C2A390>> palabras\n",
      "Vocabulario espa√±ol: <bound method Vocabulario.n_words of <__main__.Vocabulario object at 0x00000251568F3530>> palabras\n"
     ]
    }
   ],
   "source": [
    "# Crear vocabularios\n",
    "input_vocab = Vocabulario('english')\n",
    "output_vocab = Vocabulario('spanish')\n",
    "\n",
    "# Construir vocabularios con datos de entrenamiento\n",
    "for sentence in train_eng:\n",
    "    input_vocab.add_sentence(sentence)\n",
    "\n",
    "for sentence in train_esp:\n",
    "    output_vocab.add_sentence(sentence)\n",
    "\n",
    "print(f\"Vocabulario ingl√©s: {input_vocab.n_words} palabras\")\n",
    "print(f\"Vocabulario espa√±ol: {output_vocab.n_words} palabras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d2bf2",
   "metadata": {},
   "source": [
    "### 3. Preparaci√≥n de los Datos para el Modelo\n",
    "\n",
    "Hasta ahora construimos los vocabularios que nos permiten transformar palabras a √≠ndices y viceversa.\n",
    "\n",
    "El siguiente paso es convertir las oraciones a secuencias num√©ricas que pueda procesar el modelo.\n",
    "\n",
    "### Tareas para implementar\n",
    "\n",
    "1. **Funci√≥n `sentence_to_indexes(vocab, sentence)`**\n",
    "\n",
    "   - Recibe un vocabulario y una oraci√≥n en formato texto.\n",
    "   - Devuelve una lista de √≠ndices donde cada palabra de la oraci√≥n es reemplazada por su √≠ndice correspondiente en el vocabulario.\n",
    "   - Si una palabra no est√° en el vocabulario, se debe asignar el √≠ndice de un token especial para palabras desconocidas (ejemplo: `<UNK>`).\n",
    "   \n",
    "2. **Funci√≥n `indexes_to_tensor(indexes)`**\n",
    "\n",
    "   - Recibe una lista de √≠ndices.\n",
    "   - Devuelve un tensor de PyTorch de tipo `long` y con la forma esperada para ingresar al modelo.\n",
    "\n",
    "### Consideraciones\n",
    "\n",
    "- La funci√≥n `sentence_to_indexes` debe manejar el caso de palabras desconocidas de forma adecuada.\n",
    "- La funci√≥n `indexes_to_tensor` debe colocar los datos en el dispositivo correcto (CPU o GPU) si es necesario.\n",
    "\n",
    "---\n",
    "\n",
    "Estas funciones son clave para transformar nuestros datos en el formato adecuado y poder crear luego un `Dataset` y `DataLoader` que alimenten el modelo durante el entrenamiento.\n",
    "\n",
    "- Recordar que al final de cada secuencia hay agregar un token especial de fin de oraci√≥n (`EOS_token`), para que el modelo sepa cu√°ndo terminar. En nuestro caso, se hace en la creaci√≥n del dataset (`TranslationDataset`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c46fa60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indexes_aux(vocab, sentence, simbols):\n",
    "    #print(sentence)\n",
    "    ress = []\n",
    "\n",
    "    if len(simbols) == 0: # no hay simbolos especiales para indexar\n",
    "        #print(sentence.split(' '))\n",
    "        for palabra in sentence.split(' '):\n",
    "            if palabra != '':\n",
    "                if vocab.is_in_palabra(palabra):\n",
    "                    ress.append(vocab.get_index(palabra))\n",
    "                else:\n",
    "                    #print(palabra + '---')\n",
    "                    ress.append(vocab.get_index('<UNK>'))\n",
    "    else:\n",
    "        simbol = simbols[0]\n",
    "        sentencess = sentence.split(simbol)\n",
    "\n",
    "        for sen in sentencess[:-1]:\n",
    "            ress += sentence_to_indexes_aux(vocab, sen, simbols[1:])\n",
    "            ress.append(vocab.get_index(simbol))\n",
    "\n",
    "        ress += sentence_to_indexes_aux(vocab, sentencess[-1], simbols[1:])\n",
    "    return ress\n",
    "\n",
    "def sentence_to_indexes(vocab, sentence):\n",
    "    res = [vocab.get_index('SOS')]\n",
    "    \n",
    "    res += sentence_to_indexes_aux(vocab, sentence, ['.', ',', '!', '?', '¬ø','¬°'])\n",
    "\n",
    "    res.append(vocab.get_index('EOS'))\n",
    "    return res\n",
    "\n",
    "def indexes_to_sentence(vocab, indexs):\n",
    "    sentence = ''\n",
    "    for idx in indexs:\n",
    "        sentence += vocab.get_palabra(idx) + ' '\n",
    "\n",
    "    for simbol_ in [' .', ' ,', ' !', ' ?', '¬ø ',' ¬° ', '  ']:\n",
    "        sentence = sentence.replace(simbol_, simbol_[1])\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def indexes_to_tensor(indexes):\n",
    "    \"\"\"Convierte √≠ndices a tensor\"\"\"\n",
    "    return torch.tensor(indexes, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5487b9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dispuesta a mitigar mi fatiga, di unos sorbos al caf√© que me hab√≠a colocado delante. ¬øel negocio sigue mal desde aquel avistamiento?. habl√© en voz baja no quer√≠a espantar a los clientes que a√∫n quedaban en la unidad de anton. asinti√≥ con la cabeza mientras limpiaba un vaso con un trapo. t√©cnicamente este lugar es un bar, pero la verdad es que nunca vine aqu√≠ por la bebida. se escuchaban algunos murmullos por detr√°s pero, en su mayor parte, la conversaci√≥n entre anton y yo fue m√°s bien corta esta noche. me desped√≠ de √©l con un leve gesto y sal√≠ del bar con un poco m√°s de energ√≠a que antes. aaa'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = train_esp[0] + ' aaa'\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "571bc616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOS dispuesta a mitigar mi fatiga, di unos sorbos al caf√© que me hab√≠a colocado delante. el negocio sigue mal desde aquel avistamiento?. habl√© en voz baja no quer√≠a espantar a los clientes que a√∫n quedaban en la unidad de anton. asinti√≥ con la cabeza mientras limpiaba un vaso con un trapo. t√©cnicamente este lugar es un bar, pero la verdad es que nunca vine aqu√≠ por la bebida. se escuchaban algunos murmullos por detr√°s pero, en su mayor parte, la conversaci√≥n entre anton y yo fue m√°s bien corta esta noche. me desped√≠ de √©l con un leve gesto y sal√≠ del bar con un poco m√°s de energ√≠a que antes. <UNK> EOS '"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes_to_sentence(output_vocab, sentence_to_indexes(output_vocab, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bddd231e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,  3,\n",
       "         7, 25, 26, 27, 28, 29, 30, 31,  6,  3, 32, 33, 34, 35, 36, 37, 38, 11,\n",
       "        39, 40, 20, 41, 42, 33, 43, 44, 45, 46,  3, 47, 48, 43, 49, 50, 51, 52,\n",
       "        53, 48, 52, 54,  3, 55, 56, 57, 58, 52, 59,  4, 60, 43, 61, 58, 20, 62,\n",
       "        63, 64, 65, 43, 66,  3, 67, 68, 69, 70, 65, 71, 60,  4, 33, 72, 73, 74,\n",
       "         4, 43, 75, 76, 46, 77, 78, 79, 80, 81, 82, 83, 84,  3, 21, 85, 45, 86,\n",
       "        48, 52, 87, 88, 77, 89, 90, 59, 48, 52, 91, 80, 45, 92, 20, 93,  3,  9,\n",
       "         1], device='cuda:0')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes_to_tensor(sentence_to_indexes(output_vocab, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c58f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, english_sentences, spanish_sentences, input_vocab, output_vocab):\n",
    "        self.pairs = list(zip(english_sentences, spanish_sentences))\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        english_sentence, spanish_sentence = self.pairs[idx]\n",
    "        \n",
    "        # Convertir a √≠ndices\n",
    "        input_indexes = sentence_to_indexes(self.input_vocab, english_sentence)\n",
    "        target_indexes = sentence_to_indexes(self.output_vocab, spanish_sentence)\n",
    "        \n",
    "        # Agregar EOS token\n",
    "        input_indexes.append(EOS_token)\n",
    "        target_indexes.append(EOS_token)\n",
    "        \n",
    "        return {\n",
    "            'input': torch.tensor(input_indexes, dtype=torch.long),\n",
    "            'target': torch.tensor(target_indexes, dtype=torch.long),\n",
    "            'input_length': len(input_indexes),\n",
    "            'target_length': len(target_indexes)\n",
    "        }\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = TranslationDataset(train_eng, train_esp, input_vocab, output_vocab)\n",
    "val_dataset = TranslationDataset(val_eng, val_esp, input_vocab, output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9532312",
   "metadata": {},
   "source": [
    "### 4. Construcci√≥n del Modelo\n",
    "\n",
    "Ahora que ya tenemos nuestros datos listos, es momento de construir el modelo de traducci√≥n autom√°tica.\n",
    "\n",
    "Vamos a trabajar con una arquitectura **Encoder-Decoder** basada en redes recurrentes (**LSTM**, por ejemplo).\n",
    "\n",
    "En esta implementaci√≥n el decoder se alimentar√° √∫nicamente de su **propia predicci√≥n anterior** en cada paso, incluso durante el entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "üîß Qu√© deben implementar\n",
    "\n",
    "1. Encoder\n",
    "\n",
    "El encoder debe:\n",
    "\n",
    "- Tener una capa de `nn.Embedding` para convertir √≠ndices en vectores densos.\n",
    "- Tener una `nn.LSTM` (o `nn.GRU`) que procese toda la secuencia de entrada.\n",
    "- Devolver el estado oculto (`hidden`, `cell`) final del LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dfa641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO \n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         ...\n",
    "    \n",
    "#     def forward(self, input_seq):\n",
    "#         ...\n",
    "#         return hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4cd347",
   "metadata": {},
   "source": [
    "2. Decoder\n",
    "\n",
    "El Decoder es el componente encargado de **generar la secuencia de salida palabra por palabra**, usando el estado oculto final del Encoder como punto de partida.\n",
    "\n",
    "Debe contener:\n",
    "\n",
    "- Una capa de `nn.Embedding` para transformar el token de entrada en un vector denso.\n",
    "- Una LSTM que recibe el embedding y produce el siguiente estado oculto.\n",
    "- Una capa `Linear` que proyecta el estado oculto a una distribuci√≥n de probabilidad sobre el vocabulario.\n",
    "- Retornar la predicci√≥n y los nuevos estados (`hidden`, `cell`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, output_size, hidden_size):\n",
    "#         ...\n",
    "    \n",
    "#     def forward(self, input_token, hidden, cell):\n",
    "#         ...\n",
    "#         return output, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df159325",
   "metadata": {},
   "source": [
    "3. Seq2Seq\n",
    "\n",
    "La clase `Seq2Seq` orquesta la traducci√≥n completa de una secuencia. Usa un `Encoder` para codificar la entrada y un `Decoder` para generar la salida paso a paso.\n",
    "\n",
    "#### La clase debe:\n",
    "\n",
    "1. Recibir una secuencia de entrada.\n",
    "2. Usar el encoder para obtener el estado oculto inicial.\n",
    "3. Inicializar el decoder con el token **SOS**.\n",
    "4. En cada paso del tiempo:\n",
    "   - Pasar el token actual al decoder.\n",
    "   - Guardar la predicci√≥n.\n",
    "   - Usar el token m√°s probable como pr√≥ximo input.\n",
    "5. Terminar cuando:\n",
    "   - Se haya generado una cantidad fija de pasos (por ejemplo, igual al largo del target).\n",
    "   - O se haya alcanzado un m√°ximo predefinido (por ejemplo, 50 pasos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# class Seq2Seq(nn.Module):\n",
    "#     def __init__(self, encoder, decoder):\n",
    "#         ...\n",
    "    \n",
    "#     def forward(self, input_seq, max_len):\n",
    "#         ...\n",
    "#         return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84454647",
   "metadata": {},
   "source": [
    "### 5. Inicializaci√≥n y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos hiperpar√°metros\n",
    "INPUT_DIM = input_vocab.n_words      # tama√±o del vocabulario de entrada\n",
    "OUTPUT_DIM = output_vocab.n_words     # tama√±o del vocabulario de salida\n",
    "HIDDEN_DIM = 256                 # tama√±o del estado oculto\n",
    "MAX_LEN = 50                     # longitud m√°xima de la secuencia de salida\n",
    "NUM_EPOCHS = 80\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Inicializar encoder, decoder y modelo Seq2Seq\n",
    "encoder = Encoder(INPUT_DIM, HIDDEN_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, HIDDEN_DIM)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "# Funci√≥n de p√©rdida y optimizador\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_token) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(f\"Modelo creado con {sum(p.numel() for p in model.parameters())} par√°metros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Funciones auxiliares #####\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_seqs = [item['input'] for item in batch]\n",
    "    target_seqs = [item['target'] for item in batch]\n",
    "\n",
    "    # Padding\n",
    "    input_seqs = nn.utils.rnn.pad_sequence(input_seqs, batch_first=True, padding_value=PAD_token)\n",
    "    target_seqs = nn.utils.rnn.pad_sequence(target_seqs, batch_first=True, padding_value=PAD_token)\n",
    "\n",
    "    # Truncar o padear target_seqs a max_len\n",
    "    if target_seqs.size(1) > MAX_LEN:\n",
    "        target_seqs = target_seqs[:, :MAX_LEN]\n",
    "    elif target_seqs.size(1) < MAX_LEN:\n",
    "        pad_size = MAX_LEN - target_seqs.size(1)\n",
    "        padding = torch.full((target_seqs.size(0), pad_size), PAD_token, dtype=torch.long).to(target_seqs.device)\n",
    "        target_seqs = torch.cat([target_seqs, padding], dim=1)\n",
    "\n",
    "    return input_seqs.to(device), target_seqs.to(device)\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    \"\"\"Eval√∫a el modelo en el conjunto de validaci√≥n\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            output = model(input_seq=input_seq, max_len=MAX_LEN)  \n",
    "            \n",
    "            output = output.reshape(-1, output.size(-1))\n",
    "            target_seq = target_seq.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, target_seq)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    \"\"\"Entrena el modelo en un epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(input_seq, target_seq, MAX_LEN, teacher_forcing_ratio=0.5)\n",
    "        # Calcular p√©rdida\n",
    "        # Reshape para calcular cross entropy\n",
    "        output = output.reshape(-1, output.size(-1))\n",
    "        target_seq = target_seq.reshape(-1)\n",
    "\n",
    "        loss = criterion(output, target_seq)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping para evitar exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Entrenamiento del modelo #####\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Entrenamiento\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validaci√≥n\n",
    "    val_loss = evaluate_model(model, val_dataloader, criterion)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'√âpoca {epoch+1}/{NUM_EPOCHS}')\n",
    "        print(f'  P√©rdida Entrenamiento: {train_loss:.4f}')\n",
    "        print(f'  P√©rdida Validaci√≥n: {val_loss:.4f}')\n",
    "        print(f'  {\"Mejorando\" if val_loss < min(val_losses[:-1] + [float(\"inf\")]) else \"Empeorando\"}')\n",
    "\n",
    "print(\"Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c21035",
   "metadata": {},
   "source": [
    "### 6. Evaluaci√≥n y predicci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddcf6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sentence, input_vocab, output_vocab, max_length=50):\n",
    "    \"\"\"Traduce una oraci√≥n usando el modelo entrenado\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convertir oraci√≥n a tensor\n",
    "        input_indexes = sentence_to_indexes(input_vocab, sentence)\n",
    "        input_indexes.append(EOS_token)\n",
    "        input_tensor = torch.tensor(input_indexes, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Codificar\n",
    "        hidden, cell = model.encoder(input_tensor)\n",
    "        \n",
    "        # Decodificar\n",
    "        decoder_input = torch.tensor([[SOS_token]], dtype=torch.long).to(device)\n",
    "        decoded_words = []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
    "            predicted_id = output.argmax(dim=-1).item()\n",
    "            \n",
    "            if predicted_id == EOS_token:\n",
    "                break\n",
    "            \n",
    "            decoded_words.append(output_vocab.index2word[predicted_id])\n",
    "            decoder_input = torch.tensor([[predicted_id]], dtype=torch.long).to(device)\n",
    "        \n",
    "        return ' '.join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1373322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar con algunas oraciones del conjunto de validaci√≥n\n",
    "test_sentences = val_eng[:8]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTADOS DE TRADUCCI√ìN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    translation = translate(model, sentence, input_vocab, output_vocab)\n",
    "    correct_translation = val_esp[i]  # Traducci√≥n correcta correspondiente\n",
    "    \n",
    "    print(f\"Ingl√©s:     {sentence}\")\n",
    "    print(f\"Predicci√≥n: {translation}\")\n",
    "    print(f\"Correcto:   {correct_translation}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30365637",
   "metadata": {},
   "source": [
    "### 7. Probamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622023ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_translation():\n",
    "    \"\"\"Funci√≥n para probar traducciones interactivamente\"\"\"\n",
    "    print(\"\\nModo interactivo - Escribe 'quit' para salir\")\n",
    "    print(\"Nota: Solo funcionar√° con palabras que est√°n en el vocabulario de entrenamiento\")\n",
    "    print(\"Vocabulario disponible:\", list(input_vocab.word2index.keys())[3:])  # Excluir tokens especiales\n",
    "    \n",
    "    while True:\n",
    "        sentence = input(\"\\nIngresa una oraci√≥n en ingl√©s: \").strip().lower()\n",
    "        if sentence == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Verificar si todas las palabras est√°n en el vocabulario\n",
    "        words = sentence.split()\n",
    "        unknown_words = [w for w in words if w not in input_vocab.word2index]\n",
    "        \n",
    "        if unknown_words:\n",
    "            print(f\"Palabras desconocidas: {unknown_words}\")\n",
    "            print(\"Intenta con palabras del vocabulario de entrenamiento\")\n",
    "            continue\n",
    "        \n",
    "        translation = translate(model, sentence, input_vocab, output_vocab)\n",
    "        print(f\"Traducci√≥n: {translation}\")\n",
    "\n",
    "interactive_translation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f64b4",
   "metadata": {},
   "source": [
    "### 8. Ejercicios\n",
    "\n",
    "\n",
    "1. Mejoras al modelo (Opcional)\n",
    "\n",
    "Si tienen ganas, pueden explorar mejoras como:\n",
    "\n",
    "- Implementar atenci√≥n (attention mechanism).\n",
    "- Agregar m√°s capas LSTM (ajustar `NUM_LAYERS`).\n",
    "- Aplicar dropout para regularizaci√≥n.\n",
    "\n",
    "2. Responder\n",
    "\n",
    "- ¬øQu√© tipo de errores cometi√≥ su modelo en las primeras oraciones traducidas?\n",
    "- ¬øC√≥mo podr√≠an mejorar la calidad de las traducciones?\n",
    "- ¬øQu√© limitaciones ven en el enfoque actual y c√≥mo las abordar√≠an?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
